<a name="HOLTop"></a>
# Module 4 - Consuming Data Pipelines #

---

<a name="Overview"></a>
## Overview ##

Now that we have a sound data pipeline, it is time to consume our data. The data can be consumed in multiple ways. Visualizations and dashboards, reporting, bots, machine learning and other LOB apps are just a few ways that your end data can be consumed.

In this module, we will be consuming data via the AzureML Recommendations API and a Bot. We have already discussed consuming data via PowerBI in Module 1 & 2, so, we will not be covering PowerBI in this section.

The Recommendations API built with Microsoft Azure Machine Learning helps your customer discover items in your catalog. Customer activity in your digital store is used to recommend items and to improve conversion in your digital store.

The Microsot bot framework provides Conversation-as-a-Platform service. It combines natural language processing with AI to help you create conversation channels and better assist your customers.


This module includes building an Azure ML Recommendations model using the results generated by your data pipeline & using the results generated by the model to power recommendations on the e-commerce site. Additionally, we will also see how to connect a bot to a data warehouse and access the data using the Bot Framework.

<a name="Objectives"></a>
### Objectives ###
In this module, you'll see how to:

- Analyze the usersâ€™ comments using Text Analytics
- Recognize users' emotions by their faces in an image

<a name="Prerequisites"></a>
### Prerequisites ###

The following is required to complete this module:

- [Visual Studio Community 2015][1] or greater.
- [Microsoft Bot Framework Emulator][3]
- [NodeJS][2] & [MS SQL commandline utility for NodeJS][4] 

[1]: https://www.visualstudio.com/products/visual-studio-community-vs
[2]: https://nodejs.org/en/download/
[3]: https://download.botframework.com/bf-v3/tools/emulator/publish.htm
[4]: https://www.npmjs.com/package/sql-cli


<a name="Setup"></a>
### Setup ###

##### For Exercise 2 (Only to be done if you did not complete Module #2) #####
In order to work on this exercise, it is recommended that you complete module 2. However, if you have not completed module 2, please follow this setup in order to successfully complete this exercise.

>**Note:** If you've completed **Module 2**, you may **skip the setup** and start with Exercise #1.

<a name="SqlDWCreation"> </a>
#### SQL Data Warehouse Creation #####

Navigate to the Setup Folder under **Module 4**. You will find a folder called Setup\CLI. 

1. Update the parameters.json file.  Update parameters with a unique suffix for use across the services. Save the file. Particularly, you will want to update the 'uniqueSuffix' variable. This will help keep your resource names globally unique. We will be executing the azuredeploy.json ARM template, which will help us setup the resources needed for this lab.

1. Open a command prompt and navigate to the cli directory.  

1. Execute the following statements to log into your Azure subscription

	````
	azure login

	azure account list

	````

1. Copy the subscription id from the subscription to use in the lab.  Paste it in the following script and execute in the command prompt.

	````
	azure account set <subcriptionid>

	````

1. Execute the following command to create a resource group.  Use the same unique prefix from the parameters file. 

	````
	azure group create <ResourceGroupName> <Location>

	````

1. Execute the following statement to execute the ARM template that will deploy the Azure SQL  DW.  Set <DeploymentName> with the same prefix used for the ResourceGroupName.

	````
	azure group deployment create -f <path to azuredeploy.json> -e <path to parameters.json> -g <ResourceGroupName> -n <DeploymentName>

	````

1. Next, let's load up the data into our Data Warehouse. We will be using the 'bcp' tool to achieve this. You can find the data files under the folder 'Module3-Security\Setup\data'.

>**Note:** (For Mac Users) You will need to create a  blob storage account and a container within the account named 'processeddata'. Then, upload the data found in the data folder to the 'processeddata' container and follow the Exercise 2 - Tasks 1-4 in module 2 to load your data into SQL DW.

>**Note:** (For Ubuntu Users) Please follow [this][1] blog to install bcp on your machine to setup sqlcmd and bcp on your machine.

[1]: https://blogs.msdn.microsoft.com/joseph_idzioreks_blog/2015/09/13/azure-sql-database-sqlcmd-and-bcp-on-ubuntu-linux/


1. Connect to the SQL Data Warehouse using the command line tool or using Visual Studio 
	1. (For Command Line) Ensure that the SQL-CLI node package is installed on your machine.
		1. From the command line, connect to the SQL DW using the following command:
		````
		mssql -s <serverName>.database.windows.net -u <username>@<servername> -p <password> -d <databaseName> -e
		````
		1. If you used the default naming & credentials, this is what the mssql command looks like:
		````
		mssql -s readinesssqlsvr<uniqueSuffix>.database.windows.net -u labuser@readinesssqlsvr<uniqueSuffix> -p labP@ssword1 -d readinessdw -e
		````

	1. (For visual studio) Go to the Azure Portal (http://portal.azure.com) and navigate to the new SQL Data Warehouse you created.

		1. In the _SQL Data Warehouse_ blade, click **Open in Visual Studio**. In the new blade, click **Open in Visual Studio**.

			![Open Data Warehouse in Visual Studio](Images/setup-open-vs.png?raw=true "Open Data Warehouse in Visual Studio")
		
			_Open Data Warehouse in Visual Studio_

		1. Confirm you want to switch apps if prompted.

		1. In Visual Studio, enter the SQL Server credentials (labuser/labP@ssword1).

		1. In the _SQL Server Object Explorer_, expand the server and right-click the **readinessdw** database.   Select **New Query...**.


1. Enter the following commands to create a new schema for our internally managed tables. We will be 

	````SQL
	CREATE SCHEMA [adw]
	GO

	````

1.  Execute the following in a query window to create a new partitioned table.  Note this table will be created based on a SELECT statement issued on the external table. 

	````SQL
	CREATE TABLE adw.FactWebsiteActivity
	(
		EventDate datetime2,
		UserId nvarchar(20),
		Type nvarchar(20),
		ProductId nvarchar(20), 
		Quantity int, 
		Price float
	)
	WITH (
		CLUSTERED COLUMNSTORE INDEX,
		DISTRIBUTION = HASH(ProductID)
		)
	
	GO
	````

1. Similarly, let's pull our latest and greatest product catalog data from blob storage.

	````SQL
	IF EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES where TABLE_SCHEMA = 'adw' AND TABLE_NAME = 'DimProductCatalog')
		DROP TABLE adw.DimProductCatalog
	GO

	CREATE TABLE adw.DimProductCatalog
	(
		SkuNumber nvarchar(50),
		Id int,
		ProductId nvarchar(20),
		CategoryId nvarchar(20),
		CategoryName nvarchar(100),
		Title nvarchar(100),
		Price float,
		SalePrice float,
		CostPrice float
	)
	WITH (
		CLUSTERED COLUMNSTORE INDEX,
		DISTRIBUTION = HASH(ProductID)
		)
	
	````

1. Now, let's do a bulk insert into our 2 tables. Let's start with our Product Catalog table. We will use 'bcp' via the command line for this. Be sure to exit out of the SQL CLI or the sqlcmd before running bcp. Execute the following command to load the Product Catalog data.


	````
	bcp.exe adw.DimProductCatalog in ...\Module3-Security\Setup\data\product_catalog\000000_0 -S readinesssqlsvr<uniqueSuffix>.database.windows.net -d readinessdw -U labuser -P labP@ssword1 -c -q -t, -E
	````

	![Import Data into SQL DW using bcp](Images/setup-bcp.jpg?raw=true "Import Data into SQL DW using bcp")
		
			_Import Data into SQL DW using bcp_


1. Similarly, let's upload our Fact data. Since our Fact data is made up of multiple files, we'll run the bcp command multiple times.

	````
	bcp.exe adw.FactWebsiteActivity in C:\Work\Projects\oct16_upskilling\Data\Module3-Security\Setup\data\structuredlogs\000001_0 -S readinesssqlsvr<uniqueSuffix>.database.windows.net -d readinessdw -U labuser -P labP@ssword1 -c -q -t, -E

	bcp.exe adw.FactWebsiteActivity in C:\Work\Projects\oct16_upskilling\Data\Module3-Security\Setup\data\structuredlogs\000001_1 -S readinesssqlsvr<uniqueSuffix>.database.windows.net -d readinessdw -U labuser -P labP@ssword1 -c -q -t, -E
	
	bcp.exe adw.FactWebsiteActivity in C:\Work\Projects\oct16_upskilling\Data\Module3-Security\Setup\data\structuredlogs\000010_0 -S readinesssqlsvr<uniqueSuffix>.database.windows.net -d readinessdw -U labuser -P labP@ssword1 -c -q -t, -E
	
	bcp.exe adw.FactWebsiteActivity in C:\Work\Projects\oct16_upskilling\Data\Module3-Security\Setup\data\structuredlogs\000010_1 -S readinesssqlsvr<uniqueSuffix>.database.windows.net -d readinessdw -U labuser -P labP@ssword1 -c -q -t, -E
	````

1. You can verify that the data has been successfully loaded by switching back to your favorite SQL tool (VS or commandline) and executing the following SQL query.
	````SQL
	SELECT * from adw.DimProductCatalog
	GO

	SELECT count(*) from adw.FactWebsiteActivity
	GO
	````

1. Finally, let's create and populate the **adw.ProfitableProducts** table, which can be done by executing the following SQL command.

	````SQL
	IF EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES where TABLE_SCHEMA = 'adw' AND TABLE_NAME = 'ProfitableProducts')
			DROP TABLE adw.ProfitableProducts

		CREATE TABLE adw.ProfitableProducts
		WITH
		(   
			CLUSTERED COLUMNSTORE INDEX,
			DISTRIBUTION = ROUND_ROBIN
		)
		AS
		SELECT 
			a.ProductId, 
			b.CategoryName,
			SUM(a.Price - (b.CostPrice*a.Quantity)) as Profit
		FROM adw.FactWebsiteActivity AS a LEFT OUTER JOIN adw.DimProductCatalog AS b
			ON a.ProductId = b.ProductId
		WHERE  DATEDIFF(day, a.eventdate, GetDate()) < 30
		AND a.[Type]='checkout'
		GROUP BY a.ProductId, b.CategoryName;
	````


<a name="Exercise1"></a>
### Exercise 1: Adding Recommendations to your app ###

Microsoft Azure Machine Learning's Recommendations supports 3 common scenarios:
- **Frequently Bought Together (FBT) Recommendations**
	In this scenario the recommendations engine will recommend items that are likely to be purchased together in the same transaction with a particular item.

- **Item to Item Recommendations**
	A common scenario that uses this capability, is "people who visited/clicked this item, also visited/clicked this other item".

- **Customer to Item Recommendations**
	Given a customer's prior activity, it is possible to recommend items that the customer may be interested in.
	For instance, given all movies watched by a customer, it is possible to recommend additional content that may be of interest to the customer.

In this exercise, you'll set up and integrate the Product Recommendations API in your website.
You will start building a Recommendations model and then, use the results generated by the model to power recommendations on the e-commerce site.

<a name="Ex1Task1"></a>
#### Task 1 - Signing up for the Recommendations API ####

In this task, you'll sign up for the Recommendations API service, and create a recommendations model.

1. Navigate to the [Azure Portal](https://portal.azure.com) and click on **+New**.

1. Click on **Intelligence + Analytics** and click on **Cognitive Services APIs (preview)**. 

	![Setup cognitive services API](Images/setup-create-cs.png?raw=true "Cognitive services API")


1. In the creation blade, provide an Account Name. In the **API Type** section, select the **Recommendations API (preview)**.

1. Choose the **Free** pricing tier, create or select appropriate resource group and click **Create**.

	![Create cognitive services API blade](Images/setup-create-blade.png?raw=true "Create cognitive services API blade")

1. Once created, in the **Overview** section of the Recommendations API, copy the **Endpoint** and keep it handy for the next section.

1. Next, click on the **Keys** section and copy one of the two keys. Keep it handy for the next section.	

<a name="Ex1Task2"></a>
#### Task 2 - Creating a recommendations model ####

Now you, as a developer can take advantage of this API and do not have to worry about the details of how to actually build a model. You can take advantage of one that is already created.

In order to build a model, the engine will need two pieces of information, a catalog file, and a usage file. 
If you've completed Module 2 of this lab, you should have the product catalog and the transaction file handy. You can also find these resource under the '..Module4-CognitiveIntelligence\data\' folder.

>**Note:** _How much data do you need?_ 
	<br>Well, it really depends on the usage data itself. The system learns when users buy different items. For some builds like FBT, it is important to know which items are purchased in the same transactions. (We call this co-occurrences). A good rule of thumb is to have most items be in 20 transactions or more, so if you had 10,000 items in your catalog, we would recommend that you have at least 20 times that number of transactions or about 200,000 transactions. Once again, this is a rule of thumb. You will need to experiment with your data. 


1. Open in Visual Studio the **RecommendationsSample.sln** solution located at **Module4-CognitiveServices/SampleApp/RecommendationsSample.sln** folder.

1. There are 4 main files that we're going to review
	- SampleApp.cs - The main file that included the application code. 
	- RecommendationsApiWrapper.cs - A wrapper that makes it easy to consume the recommendations API from C# 
	- Helpers.cs - Helper file with classes to simplify serialization/deserialization of the RESTful API requests/responses. 
	- BlobHelper.cs - Helper File to deal with Blob Storage. (Used only for batch scoring)
	
<br>
1. Open the **SampleApp.cs** file. The following tasks are going to be executed:
	- Create a Model
	- Add a catalog file
	- Add a usage file
	- Trigger a build for the Model
	- Get a recommendation based on a pair of items

1. Replace the values for the **AccountKey** and **BaseUri** fields with your key and Endpoint from the previous task.

1. You will need Product Catalog data and the Usage data. You can find that in the '..\Module4-Consumption\data' folder. Alternatively, if you've completed **Module 2** of this lab series, you would have written a hive query to output the data into hive tables named **RecUsageData** & **RecProductCatalog**. This data would be stored in the blob storage account you chose for your HDInsight cluster. You can navigate to the Storage account and HDInsight container and make your way to the **./hive/warehouse/<**table name folder**>** to find the data. You do not need your HDInsight cluster to be running in order to access this data.

1. As mentioned earlier, in order to train the ML Recommendations model, we need to upload our Product Catalog and our website's usage data. This data was produced by output in Module 2. We will be consuming this output in order to train out model.

1. The Recommendations model expects the data to be in a certain format. All files need to be comma (,) separated.
	1. The following is the format the **Product Catalog** data needs to be in:
		- Product ID
		- Product Title
		- Category ID
		- Description (Optional)
		- Comma separated list of features (Optional)
	
	1. The following in the format the **Usage Data** needs to be in:
		- User ID
		- Product ID
		- Time (Format: yyyy/MM/ddThh:mm:ss)
		- Event (Purchase/Click/AddShopCart/RemovateShopCart)

1. Let's open the data files and see what the data looks like.

	- Product Catalog Data
		![Product catalog file](Images/ex1-product-catalog-file.png?raw=true "Product catalog file")

	- Usage Data
		![Usage data file](Images/ex1-usage-data-file.png?raw=true "Usage data file")


1. Let's switch back to our visual studio solution and add the values for **ProductCatalogPath** and the **UsageFilesPath** variables with the folder paths of the product catalog and the usage data folders, respectively.

1. Run the solution and take note of the **Model Id** that is shown when the application finishes, you'll need it for the following task.

	![Console output](Images/ex1-console-output.png?raw=true "Console output")

<a name="Ex1Task3"></a>
#### Task 3 - Viewing recommendations in action ####

In this task, you'll view your recommendation model in action by viewing them on a website, which calls the recommendations API.

1. Open in Visual Studio the **RecViewerApp.sln** solution located at **Module4-Consumption / RecViewerApp** folder.

1. Run the application. Once it loads, enter your Recommendations API **account key** and select your **ModelID**.

1. Once you fill the information in, click on any of the products shown under the **Products** section.

	![Products on our mock-up Website](Images/ex1-view-products.png?raw=true "Products on our mock-up Website")

	_Products on our mock-up Website_



1. You should see Recommendations show up for each product that you click on.

	![Recommendations for an item](Images/ex1-rec-view-wo-details.png?raw=true "Recommendations for an item")

	_Recommendations for an item_


1. Now, click on any recommended item and you will notice that it will also display the **confidence %** and the **reasoning** why it recommended the item.
	![Recommendations With Reasoning](Images/ex1-rec-view.png?raw=true "Recommendations With Reasoning")

	_Recommendations for an item With Reasoning_

	1. You can easily filter out recommended items that are below a certain threshold.

	1. You can also notice that the reasoning has the words "People who **like this** also **like this**" or "People who **buy this** also **buy this**". Generally, if there is a strong **buying** correlation pattern between two products, you will get the latter reasoning. If there is not a strong buying correlation pattern, you will receive the first reasoning. Since our data is auto-generated, it is less likely that you will see any buying correlations in the recommendations.

1. Let's examine the code. Go to the Visual Studio solution and open up the **Default.aspx.cs** file. Examine the method called **ModelKey_TextChanged**. This method calls the **GetModels()** method that returns all the models for the Recommendation API key.

1. Next, examine the **productdetails_ItemCommand** method. This method calls the **GetRecommendations** method, which in-turn returns recommendations for any selected product.

1. Finally, open the **RecommendationsApiWrapper.cs** file. This file contains the different methods required to interact with the Recommendations API. This class is available as a part of the Sample App that can be downloaded via the Azure documentation [here](https://github.com/microsoft/Cognitive-Recommendations-Windows)

### Appendix ###

1. [Recommendations API - Developer Reference](https://westus.dev.cognitive.microsoft.com/docs/services/Recommendations.V4.0/operations/56f30d77eda5650db055a3db)

1. [Recommendation API - Documentation](https://azure.microsoft.com/en-us/documentation/articles/machine-learning-recommendation-api-documentation/)


1. [Getting Started with Recommendations API](https://azure.microsoft.com/en-us/documentation/articles/cognitive-services-recommendations-quick-start/)




<a name="Exercise2"></a>
### Exercise 2: Powering your Bot using Data ###

In this exercise, we will use the Bot framework to connect our data to make our bot intelligent.

>**Note:** We will not be utilizing [LUIS](https://luis.ai) for the bot built in this lab. This lab is meant to be a showcase of how bots are yet another application connecting to your data source and how you can add smarts to your bot by connecting it to your analytical data store.

#### Task 1: Understanding the Code ####

1. Open in Visual Studio the **DBAccessBotDemo.sln** solution located at **Module4-Consumption / DBAccessBotDemo** folder.

1. Open the file **MessagesController.cs** found in the **Controller** folder. 

1. Enter the ADO.NET SQL DW Connection String for the private variable called **connectionString**. Your connection string should look as follows:

	````
	Server = tcp:<server name>.database.windows.net,1433; Database = <data warehouse name>; User ID = <username>; Password = <password>; Encrypt = True; TrustServerCertificate = False; Connection Timeout = 30;
	````

1. If you used the default values in the ARM template to create the SQL Server and the Data Warehouse, the connection string should look similar to the one below:

	````
	Server = tcp:readinesssqlsvr<uniqueSuffx>.database.windows.net,1433; Database = readinessdw; User ID = labuser; Password = labP@ssword1; Encrypt = True; TrustServerCertificate = False; Connection Timeout = 30;
	````

1. Notice the **StartAsync** method. This method allows you to have a dialogue with the bot. This method calls the **MessageReceivedAsync** method.

1. In the **MessageReceivedAsync** method, we are creating a connection to our SQL DW using the connection string.

	````
	//Connect to DataWarehouse and put content of query into reader
            using (var connection = new SqlConnection(connectionString)) //Call out
            {
                connection.Open();
	````

1. At the end of the **MessageReceivedAsync** method, notice how we're again calling the same method, thus ensuring that our program does not end after the first message has been sent.

		````
		context.Wait(MessageReceivedAsync)
		````


1. Once our connection is created and opened, it's now time to send a query to our database and receive a result back. It is **Very Important** to note that we should **ONLY** connect to **Analytical** data stores and **not** transactional data stores for querying of our data. In this case, the SQL Data Warehouse acts as our **Analytical Data Store** and hence we're connecting to it to fetch our results.

1. We've created three (3) intents for this lab. All of them help us get information about our e-commerce store using our SQL Data Warehouse table.
	1. Identifying Profit for a Specified Product
		- In this case, we're submitting questions to the bot such as "**Show me the profit for product: Filter Set**". This intent should fire a query to the Data Warehouse and should pull down the profits for the product titled **Filter Set**.
	1. 	Products making profit lesser than a certain threshold
		- Here, the inten is to understand if there are certain products that are not making enough profit. Here, we're expecting questions such as "**Show me products that have made profits of less than 20000**". This intent should execute a query to give you products whose profits are less than the 20000 threshold. 
	1. Most Profitable Products
		- : The intent here is to understand the store's highest profiting product. The questions that we're expecting here are of the type "**Show me the most profitable product**".

>**Note:** In a real-world scenario, the aforementioned intents would be handled and identified using [LUIS](https://luis.ai). However, in this case, we have used _if_ statements to filter the question and identify the intent.


#### Task 2: Writing the SQL queries to pull the data ####

We will be connecting to the SQL table **adw.ProfitableProducts** to pull down the results.

1. In the code, review how we submit a SQL Query to the SQL DW and how we read the output of that query. We're using **GetString(ColumnNumber)** and **GetDouble(ColumnNumber)** to cast and handle our output data.
	````
	using (var Command = new SqlCommand())
    {
        Command.Connection = connection;
        Command.CommandType = System.Data.CommandType.Text; 
        Command.CommandText = @"select * from dbo.MyTable";

        SqlDataReader reader = Command.ExecuteReader();

		while (reader.Read())
        {
			Console.WriteLine(String.Format("Profit for {0}: ${1}", reader.GetString(0), reader.GetDouble(1).ToString()));
		}
	}
	````

1. Now, let's write our SQL Queries for the intents that we have defined.

	1. Identifying profit for a specified product
		- In this case, the query would look something like follows. Notice how the **title** field is a variable which is being populated by the end user's input.
		
		````
		select a.Profit, b.title from adw.ProfitableProducts a INNER JOIN adw.DimProductCatalog b ON a.productId=b.productId WHERE b.title='" + title + "' 
		````
		- _Explanation of the Query_: This query joins the ProfitableProducts table with our DimProductCatalog table to fetch the profit for each product along with the product titles and then filters on the specific requested title using the WHERE clause.
		 
		- Let's paste this query in the section that asks you to insert **Query1**
		
			!Query 1 to pull profits for a specific product](Images/ex2-task2-q1.png?raw=true "Query 1 to pull profits for a specific product")
		
			_Query 1 to pull profits for a **specific** product_
				


	1.  Products making profit lesser than a certain threshold
		- Here, we would query the database to check for products with profit<_threshold_. The query would be as follows. Notice how the **threshold** field gets populated dynamically.
		````
		select a.*, b.title from adw.ProfitableProducts a INNER JOIN adw.DimProductCatalog b ON a.productId=b.productID WHERE a.Profit <'" + threshold + "' 
		````
		- _Explanation of the Query_: This query joins the ProfitableProducts table with our DimProductCatalog table to fetch the profit for each product along with the product titles and then filters out the products that have profits less than the _threshold_ using the WHERE clause.
		
		- Let's paste this query in the section that asks you to insert **Query2**
		
			![Query 2 to pull products less than a certain threshold amount](Images/ex2-task2-q2.png?raw=true "Query 2 to pull products less than a certain threshold amount")

			_Query 2 to pull products **less than a certain threshold** amount_


	1. Most Profitable Products
		- In this scenario, we want to find out our most profitable product. For this, the query would look as follows:
		````
		select top 1 a.*, b.title from adw.ProfitableProducts a INNER JOIN adw.DimProductCatalog b ON a.productId=b.productID ORDER BY a.Profit DESC
		````
		- _Explanation of the Query_: This query joins the ProfitableProducts table with our DimProductCatalog table to fetch the profit for each product along with the product titles and then ORDERS products according to their profits so that we have out most profitable product as row #1. Then, we use the **Top 1** filter to only fetch the first row of this select query.
		
		- Let's paste this query in the section that asks you to insert **Query3**
			
			![Query 3 to pull most profitable products](Images/ex2-task2-q3.png?raw=true "Query 3 to pull most profitable products")

			_Query 3 to pull **most profitable** products_
		

1. Finally, notice how we are exposing the results out to the client using the **context.PostAsync()** method.
		
	![Returning the output back to the client](Images/ex2-task2-result.png?raw=true "Returning the output back to the client")

	_Returning the output back to the client_


#### Task 3: See the bot in action ####

1. Let's execute this program. Once executed, it should open a tab in your browser and take you to a **localhost** address. Copy that address.

1. Now, open the **Bot Framework Emulator** program.

1. On the top, under the section **Bot Url**, please paste the following url. Please be sure to replace the port number with the port number you copied in the previous step. Now you are ready to interact with the bot.

	````
	http://localhost:<port number>/api/messages
	````
	![The Bot Url in the Emulator](Images/ex2-task3-boturl.png?raw=true "The Bot Url in the Emulator")

	_The Bot Url in the Emulator_

1. Let's test our scenarios.

	1. Let's type the question: **what is the profit for product: Filter Set**. Ensure that you do not miss the colon(:) and that you only have the product title name after the colon. This will ensure that we can parse the question on the backend correctly. Your output should look as follows:
		
		![Bot responding to Question #1](Images/ex2-task3-q1.png?raw=true "Bot responding to Question #1")

		_Bot responding to Question #1_

	1. Next Question: **show me products that have made profit less than 20000**. Ensure that you put the threshold number at the very end of the sentence and you either use the phrase _less than_ or use the symbol (<).
		![Bot responding to Question #2](Images/ex2-task3-q2.png?raw=true "Bot responding to Question #2")

		_Bot responding to Question #2_

	1. Last one: **show me the most profitable product**. Ensure that you use the phrase _most profitable_ for the command to work.
	

		![Bot responding to Question #3](Images/ex2-task3-q3.png?raw=true "Bot responding to Question #3")

		_Bot responding to Question #3_

With this, we have successfully connected our bot to our Data warehouse and created a smart bot, which is always able to answer questions about our profitable products based on the most latest and accurate data.